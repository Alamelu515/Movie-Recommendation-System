{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **RECOMMENDATION SYSTEM 1: Using Neural Collaborative Filtering (NCF) Model**"
      ],
      "metadata": {
        "id": "Gi8Sq6wGNYZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and Import Required Libraries:"
      ],
      "metadata": {
        "id": "bygC6qsh8te8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9CLSX9aw8rpK"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Data:"
      ],
      "metadata": {
        "id": "-Wy_ZTcR8y4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
        "\n",
        "# Select the basic features.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"]\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])\n"
      ],
      "metadata": {
        "id": "K_RJN_BL81bn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Vocabularies:"
      ],
      "metadata": {
        "id": "Mi00bQam83Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
        "\n",
        "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "movie_titles_vocabulary.adapt(movies)\n"
      ],
      "metadata": {
        "id": "X0bepCHn87DW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the NCF Model:"
      ],
      "metadata": {
        "id": "OOZ-9qwa88ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NCFModel(tfrs.Model):\n",
        "    def __init__(self, user_model: tf.keras.Model, movie_model: tf.keras.Model, task: tfrs.tasks.Retrieval):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.movie_model = movie_model\n",
        "        self.task = task\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        user_embeddings = self.user_model(features[\"user_id\"])\n",
        "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "        return self.task(user_embeddings, movie_embeddings)\n"
      ],
      "metadata": {
        "id": "FBQxdRAH9IWJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define User and Movie Models with NCF Architecture:"
      ],
      "metadata": {
        "id": "tLy_7-7g9Kkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "\n",
        "user_model = tf.keras.Sequential([\n",
        "    user_ids_vocabulary,\n",
        "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), embedding_dim),\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu')\n",
        "])\n",
        "\n",
        "movie_model = tf.keras.Sequential([\n",
        "    movie_titles_vocabulary,\n",
        "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), embedding_dim),\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13VH97BT9Li0",
        "outputId": "bdcb7b63-db00-4628-e602-54394de43958"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Retrieval Task:"
      ],
      "metadata": {
        "id": "2ZlnTiG_9Nhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = tfrs.tasks.Retrieval(\n",
        "    metrics=tfrs.metrics.FactorizedTopK(\n",
        "        movies.batch(128).map(movie_model)\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "sm14-QF99Psj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and Train the Model:"
      ],
      "metadata": {
        "id": "MfTPyBxL9ROe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retrieval model.\n",
        "model = NCFModel(user_model, movie_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
        "\n",
        "# Train for 3 epochs.\n",
        "model.fit(ratings.batch(4096), epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuSn4EhQ9URI",
        "outputId": "d8384696-8f77-4aca-a88b-b013c51c8388"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "25/25 [==============================] - 23s 845ms/step - factorized_top_k/top_1_categorical_accuracy: 0.6304 - factorized_top_k/top_5_categorical_accuracy: 0.6349 - factorized_top_k/top_10_categorical_accuracy: 0.6374 - factorized_top_k/top_50_categorical_accuracy: 0.6466 - factorized_top_k/top_100_categorical_accuracy: 0.6542 - loss: 609585.7176 - regularization_loss: 0.0000e+00 - total_loss: 609585.7176\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 8s 334ms/step - factorized_top_k/top_1_categorical_accuracy: 0.9530 - factorized_top_k/top_5_categorical_accuracy: 0.9535 - factorized_top_k/top_10_categorical_accuracy: 0.9537 - factorized_top_k/top_50_categorical_accuracy: 0.9551 - factorized_top_k/top_100_categorical_accuracy: 0.9566 - loss: 32460.4747 - regularization_loss: 0.0000e+00 - total_loss: 32460.4747\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 9s 346ms/step - factorized_top_k/top_1_categorical_accuracy: 0.9644 - factorized_top_k/top_5_categorical_accuracy: 0.9648 - factorized_top_k/top_10_categorical_accuracy: 0.9650 - factorized_top_k/top_50_categorical_accuracy: 0.9661 - factorized_top_k/top_100_categorical_accuracy: 0.9671 - loss: 32430.1847 - regularization_loss: 0.0000e+00 - total_loss: 32430.1847\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ed5b7800400>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate and Get Recommendations:"
      ],
      "metadata": {
        "id": "8IsDASJu9WWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use brute-force search to set up retrieval using the trained representations.\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "index.index_from_dataset(\n",
        "    movies.batch(100).map(lambda title: (title, model.movie_model(title))))\n",
        "\n",
        "# Get some recommendations.\n",
        "_, titles = index(np.array([\"42\"]))\n",
        "print(\"Top 5 recommendations for user 42:\")\n",
        "for i, title in enumerate(titles[0, :5], 1):\n",
        "    print(f\"{i}. {title.numpy().decode('utf-8')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu1Ju2ZB9WD-",
        "outputId": "b45c359a-ab8d-47ab-ef87-36789e3c202c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommendations for user 42:\n",
            "1. You So Crazy (1994)\n",
            "2. Love Is All There Is (1996)\n",
            "3. Fly Away Home (1996)\n",
            "4. In the Line of Duty 2 (1987)\n",
            "5. Niagara, Niagara (1997)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RECOMMENDATION SYSTEM 2: Using Bayesian Personalized Ranking (BPR) Model**"
      ],
      "metadata": {
        "id": "_QZXigwFPbpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and Import Required Libraries:"
      ],
      "metadata": {
        "id": "Odz18s6tPbmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs\n"
      ],
      "metadata": {
        "id": "_LvgGG0pQ9sq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Data:"
      ],
      "metadata": {
        "id": "F_kiCmk0Po3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
        "\n",
        "# Select the basic features.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"]\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])\n"
      ],
      "metadata": {
        "id": "_WCg-WZcQ7pL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Vocabularies:"
      ],
      "metadata": {
        "id": "FsY_qIiiPtB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
        "\n",
        "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "movie_titles_vocabulary.adapt(movies)\n"
      ],
      "metadata": {
        "id": "H7MfDrO-Q5lg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the BPR Model:"
      ],
      "metadata": {
        "id": "gBuloyDBP4un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BPRModel(tfrs.Model):\n",
        "    def __init__(self, user_model: tf.keras.Model, movie_model: tf.keras.Model, task: tfrs.tasks.Retrieval):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.movie_model = movie_model\n",
        "        self.task = task\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        user_embeddings = self.user_model(features[\"user_id\"])\n",
        "        positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "\n",
        "        # Generate negative samples\n",
        "        negative_movie_embeddings = self.movie_model(tf.random.shuffle(features[\"movie_title\"]))\n",
        "\n",
        "        # Calculate the BPR loss\n",
        "        positive_scores = tf.reduce_sum(user_embeddings * positive_movie_embeddings, axis=1)\n",
        "        negative_scores = tf.reduce_sum(user_embeddings * negative_movie_embeddings, axis=1)\n",
        "        bpr_loss = -tf.reduce_mean(tf.math.log(tf.nn.sigmoid(positive_scores - negative_scores)))\n",
        "\n",
        "        return self.task(user_embeddings, positive_movie_embeddings) + bpr_loss\n"
      ],
      "metadata": {
        "id": "YhuiwK4MQ3t4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define User and Movie Models:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lm5_CvvRQFQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "\n",
        "user_model = tf.keras.Sequential([\n",
        "    user_ids_vocabulary,\n",
        "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), embedding_dim)\n",
        "])\n",
        "\n",
        "movie_model = tf.keras.Sequential([\n",
        "    movie_titles_vocabulary,\n",
        "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), embedding_dim)\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8N2j8o5Q1v2",
        "outputId": "84b394d7-3df4-476e-9929-c02f52cfb7fc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Retrieval Task:"
      ],
      "metadata": {
        "id": "FU4qHkc9QQkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = tfrs.tasks.Retrieval(\n",
        "    metrics=tfrs.metrics.FactorizedTopK(\n",
        "        movies.batch(128).map(movie_model)\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "Wy86UMDtQzzg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and Train the Model:"
      ],
      "metadata": {
        "id": "NvoN35L9QRO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retrieval model.\n",
        "model = BPRModel(user_model, movie_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
        "\n",
        "# Train for 3 epochs.\n",
        "model.fit(ratings.batch(4096), epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOIOEkWfQx93",
        "outputId": "60877372-bed4-46e9-f1de-f5ff26fb5983"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "25/25 [==============================] - 25s 984ms/step - factorized_top_k/top_1_categorical_accuracy: 7.0000e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0014 - factorized_top_k/top_10_categorical_accuracy: 0.0046 - factorized_top_k/top_50_categorical_accuracy: 0.0436 - factorized_top_k/top_100_categorical_accuracy: 0.0979 - loss: 33102.0699 - regularization_loss: 0.0000e+00 - total_loss: 33102.0699\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 15s 580ms/step - factorized_top_k/top_1_categorical_accuracy: 1.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0049 - factorized_top_k/top_10_categorical_accuracy: 0.0140 - factorized_top_k/top_50_categorical_accuracy: 0.1056 - factorized_top_k/top_100_categorical_accuracy: 0.2109 - loss: 31011.8137 - regularization_loss: 0.0000e+00 - total_loss: 31011.8137\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 15s 588ms/step - factorized_top_k/top_1_categorical_accuracy: 4.1000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0082 - factorized_top_k/top_10_categorical_accuracy: 0.0220 - factorized_top_k/top_50_categorical_accuracy: 0.1448 - factorized_top_k/top_100_categorical_accuracy: 0.2693 - loss: 30418.5863 - regularization_loss: 0.0000e+00 - total_loss: 30418.5863\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ed5b7933a60>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate and Get Recommendations:"
      ],
      "metadata": {
        "id": "XQ21c2tKQWV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use brute-force search to set up retrieval using the trained representations.\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "index.index_from_dataset(\n",
        "    movies.batch(100).map(lambda title: (title, model.movie_model(title))))\n",
        "\n",
        "# Get some recommendations.\n",
        "_, titles = index(np.array([\"42\"]))\n",
        "print(\"Top 5 recommendations for user 42:\")\n",
        "for i, title in enumerate(titles[0, :5], 1):\n",
        "    print(f\"{i}. {title.numpy().decode('utf-8')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvVhATTmQxPE",
        "outputId": "ebb1e251-a6e7-4db6-9d6b-af1093003dac"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommendations for user 42:\n",
            "1. Rent-a-Kid (1995)\n",
            "2. Aristocats, The (1970)\n",
            "3. Just Cause (1995)\n",
            "4. House Arrest (1996)\n",
            "5. Winnie the Pooh and the Blustery Day (1968)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RECOMMENDATION SYSTEM 3: Using Graph Neural Network (GNN) Model**"
      ],
      "metadata": {
        "id": "d54D7RjoP85q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and Import Required Libraries:"
      ],
      "metadata": {
        "id": "fjgN5ejtPnVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs\n"
      ],
      "metadata": {
        "id": "_ZVJBaG3QqeG"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Data:"
      ],
      "metadata": {
        "id": "z21NRNt1PrsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
        "\n",
        "# Select the basic features.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"]\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])\n"
      ],
      "metadata": {
        "id": "4irRlwgBQpzg"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Vocabularies:"
      ],
      "metadata": {
        "id": "2cH6WgiaPySa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
        "\n",
        "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "movie_titles_vocabulary.adapt(movies)\n"
      ],
      "metadata": {
        "id": "_MIxWRLaQmeq"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the GNN Model:"
      ],
      "metadata": {
        "id": "UokV3GvkP71E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(tfrs.Model):\n",
        "    def __init__(self, user_model: tf.keras.Model, movie_model: tf.keras.Model, task: tfrs.tasks.Retrieval):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.movie_model = movie_model\n",
        "        self.task = task\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        user_embeddings = self.user_model(features[\"user_id\"])\n",
        "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "        return self.task(user_embeddings, movie_embeddings)\n"
      ],
      "metadata": {
        "id": "38CQQ3tbQkHv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define User and Movie Models with GNN Layers:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RMbPgfd2QI12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "\n",
        "class GNNLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.weight = self.add_weight(shape=(input_shape[-1], self.units), initializer=\"random_normal\", trainable=True)\n",
        "        self.bias = self.add_weight(shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.relu(tf.matmul(inputs, self.weight) + self.bias)\n",
        "\n",
        "user_model = tf.keras.Sequential([\n",
        "    user_ids_vocabulary,\n",
        "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), embedding_dim),\n",
        "    GNNLayer(embedding_dim)\n",
        "])\n",
        "\n",
        "movie_model = tf.keras.Sequential([\n",
        "    movie_titles_vocabulary,\n",
        "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), embedding_dim),\n",
        "    GNNLayer(embedding_dim)\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyMTkWbSQjCC",
        "outputId": "26f71fc0-cf89-48e0-85d7-0447fe0a4cdd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
            "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Retrieval Task:"
      ],
      "metadata": {
        "id": "6cP0I7nyQNtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = tfrs.tasks.Retrieval(\n",
        "    metrics=tfrs.metrics.FactorizedTopK(\n",
        "        movies.batch(128).map(movie_model)\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "s6lEFf-HQfhA"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and Train the Model:"
      ],
      "metadata": {
        "id": "QwAGeUEfQVf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retrieval model.\n",
        "model = GNNModel(user_model, movie_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
        "\n",
        "# Train for 3 epochs.\n",
        "model.fit(ratings.batch(4096), epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVDoh8QnQdDk",
        "outputId": "cbf693ab-f31f-4be4-c956-3167ab2a882f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "25/25 [==============================] - 24s 891ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1209 - factorized_top_k/top_5_categorical_accuracy: 0.3009 - factorized_top_k/top_10_categorical_accuracy: 0.3086 - factorized_top_k/top_50_categorical_accuracy: 0.3783 - factorized_top_k/top_100_categorical_accuracy: 0.4281 - loss: 923564.4526 - regularization_loss: 0.0000e+00 - total_loss: 923564.4526\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 10s 412ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2695 - factorized_top_k/top_5_categorical_accuracy: 0.5209 - factorized_top_k/top_10_categorical_accuracy: 0.5253 - factorized_top_k/top_50_categorical_accuracy: 0.6866 - factorized_top_k/top_100_categorical_accuracy: 0.7719 - loss: 33011.7733 - regularization_loss: 0.0000e+00 - total_loss: 33011.7733\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 10s 390ms/step - factorized_top_k/top_1_categorical_accuracy: 0.5879 - factorized_top_k/top_5_categorical_accuracy: 0.6123 - factorized_top_k/top_10_categorical_accuracy: 0.6149 - factorized_top_k/top_50_categorical_accuracy: 0.8055 - factorized_top_k/top_100_categorical_accuracy: 0.8383 - loss: 32475.9237 - regularization_loss: 0.0000e+00 - total_loss: 32475.9237\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ed5b7932da0>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate and Get Recommendations:"
      ],
      "metadata": {
        "id": "HrohKbxYQYtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use brute-force search to set up retrieval using the trained representations.\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "index.index_from_dataset(\n",
        "    movies.batch(100).map(lambda title: (title, model.movie_model(title))))\n",
        "\n",
        "# Get some recommendations.\n",
        "_, titles = index(np.array([\"42\"]))\n",
        "print(\"Top 5 recommendations for user 42:\")\n",
        "for i, title in enumerate(titles[0, :5], 1):\n",
        "    print(f\"{i}. {title.numpy().decode('utf-8')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io9sE_KLQaYN",
        "outputId": "67dd6dbd-a11e-4f39-e22a-55af73b26814"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommendations for user 42:\n",
            "1. Star Trek: First Contact (1996)\n",
            "2. Herbie Rides Again (1974)\n",
            "3. Life Less Ordinary, A (1997)\n",
            "4. Lion King, The (1994)\n",
            "5. Last Time I Saw Paris, The (1954)\n"
          ]
        }
      ]
    }
  ]
}